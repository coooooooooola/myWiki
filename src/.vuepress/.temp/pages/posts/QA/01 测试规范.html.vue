<template><div><h3 id="_2-1-测试前" tabindex="-1"><a class="header-anchor" href="#_2-1-测试前"><span>2.1 测试前：</span></a></h3>
<ul>
<li>需求评审：</li>
</ul>
<p>理解业务是第一要义，在梳理清楚需求点以及交互后，可以尝试做下面的事情（挑重点需求，毕竟没那么多时间）：思考如果是自己来设计技术方案该怎么做（包含不限于 ① 主流程的时序图 ② 涉及的接口，每个接口要处理的核心逻辑以及用到的中间件 ③ 会有哪些异常场景 ④ 自己的方案可能会产生或者无法解决什么问题）。</p>
<p>我认为这个过程很有帮助：1）主动思考后产生的想法和问题，会让后面的技术评审、用例编写等环节更充分有效，不会变成 QA/RD 一方的表演，会碰撞出很多隐藏问题 2）不断提升自己的技术能力与业务理解，一方面可以发现更多系统设计上的缺陷 提升代码质量，另一方面大部分的测试平台会使用与业务一致的技术栈，在平台开发时更加得心应手 3）与研发可以平等交流，增进信任度。很多业务线研发认为 QA 只会点点点，往往并不愿意与 QA 进行很深入的交流，QA 在推动质量专项时也会很被动</p>
<ul>
<li>技术评审：</li>
</ul>
<p>在这个环节 QA 需要搞定这几件事：1）熟悉 RD 的技术方案，先看技术方案完备度（应该有一个技术方案模板） 2）主流程是否遗漏或存在逻辑漏洞 3）结合自己此前的思考，将疑问与 RD 交流</p>
<ul>
<li>用例编写：</li>
</ul>
<p>我一般写服务端测试用例的思路是：</p>
<ol>
<li>【新】依照时序图，将一个完整流程按拆分为独立的多个独立模块/接口</li>
<li>【新】对于每个模块/接口，列出接口文档、接口核心改动逻辑、数据存储&amp;中间件校验点以及异常场景，标注优先级</li>
<li>【回归】反模式补充：结合之前踩过的坑以及线上问题，补充 case</li>
<li>【回归】主流程：结合测试模板，加入主流程的回归 case</li>
<li>在这个过程中，如果对技术方案或者需求有些疑问，阻塞的即使沟通，不阻塞的在用例评审时沟通</li>
<li>脑海里提前过一遍测试可能遇到的可测性问题，给一个初版解决方案</li>
</ol>
<ul>
<li>用例评审：</li>
</ul>
<p>提前发出自己的用例，评审时 P0/P1 逐条过，P2/P3 如果太多可以简要过（毕竟精力有限，如果上百条无关紧要的 case 也要挨个念，RD 可能会抓不到重点），将自己的疑问都沟通明白</p>
<ul>
<li>准入：这没啥说的，按照准入标准（自动化、冒烟用例、静态扫描、FTF）缺一不可</li>
</ul>
<h3 id="测试中" tabindex="-1"><a class="header-anchor" href="#测试中"><span>测试中：</span></a></h3>
<p>依照测试用例严格执行（测试 Case 需要有明确的校验点和可执行性），测试过程中可能用到的方法：</p>
<ul>
<li>postman、charles 等接口工具，也有 rpc 转 http 的工具</li>
<li>代码 debug</li>
<li>Mock 数据（上下游、系统内、第三方）</li>
<li>端到端测试</li>
<li>日志分析</li>
<li>代码覆盖率</li>
<li>使用各种中间件工具（DB、MQ、Cache、ES……）</li>
</ul>
<h3 id="测试后" tabindex="-1"><a class="header-anchor" href="#测试后"><span>测试后</span></a></h3>
<ol>
<li>
<p>准出：</p>
<p>自动化+FTF+行覆盖率+代码扫描+缺陷修复率+用例执行率+是否通过代码 review</p>
</li>
<li>
<p>上线：</p>
<p>分级发布，观察日志，及时回归</p>
</li>
<li>
<p>补充核心功能自动化用例、压测…</p>
</li>
</ol>
</div></template>


